{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Library Tools\n",
    "\n",
    "For use with a Pyspark backend (or fakespark, not fully supported yet)\n",
    "\n",
    "Use of notebooks in spark requires appropriate environment variables to be set\n",
    "\n",
    "```bash\n",
    "PYSPARK_DRIVER_PYTHON=jupyter PYSPARK_DRIVER_PYTHON_OPTS=notebook PYSPARK_PYTHON=/Users/mader/anaconda/bin/python /Volumes/ExDisk/spark-2.0.0-bin-hadoop2.7/bin/pyspark\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from skimage.io import imread\n",
    "import pandas as pd\n",
    "from io import BytesIO, StringIO\n",
    "from pyspark import SQLContext\n",
    "\n",
    "from dicom import read_file\n",
    "class PyqaeContext(object):\n",
    "    \"\"\"\n",
    "    The primary context for performing PYQAE functions\n",
    "    \"\"\"\n",
    "    def __init__(self, cur_sc = None, faulty_io = 'FAIL', retry_att = 5, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Create or initialize a new Pyqae Context\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        cur_sc : SparkContext\n",
    "            An existing initialized SparkContext, if none a new one is initialized with the other parameters.\n",
    "        faulty_io : String\n",
    "            A string indicating what should happen if a file is missing (FAIL, RETRY, or return an EMPTY value)\n",
    "        retry_att : Int\n",
    "            The number of times a retry should be attempted (if faulty_io is in mode RETRY otherwise ignored)\n",
    "        \"\"\"\n",
    "        assert faulty_io in ['FAIL', 'RETRY', 'EMPTY'], \"Faulty IO must be in the list of FAIL, RETRY, or EMPTY\"\n",
    "        assert retry_att>0, \"Retry attempt must be greater than 0\"\n",
    "        self.faulty_io = faulty_io\n",
    "        self.retry_att = retry_att\n",
    "        if cur_sc is None: \n",
    "            from pyspark import SparkContext\n",
    "            self._cur_sc = SparkContext(*args, **kwargs)\n",
    "        else:\n",
    "            self._cur_sc = cur_sc\n",
    "    \n",
    "    @staticmethod\n",
    "    def _wrapIOCalls(method, faulty_io, retry_att):\n",
    "        \"\"\"\n",
    "        A general wrapper for IO calls which should be retried or returned empty \n",
    "        \n",
    "        \"\"\"\n",
    "        assert faulty_io in ['FAIL', 'RETRY', 'EMPTY']\n",
    "        assert retry_att > 0, \"Retry attempts should be more than 0, {}\".format(retry_att)\n",
    "        if faulty_io == 'FAIL':\n",
    "            return method\n",
    "        else:\n",
    "            def wrap_method(*args, **kwargs):\n",
    "                if faulty_io == 'RETRY': max_iter = retry_att-1\n",
    "                else: max_iter = 1\n",
    "                \n",
    "                for i in range(max_iter):\n",
    "                    try:\n",
    "                        return method(*args,**kwargs)\n",
    "                    except:\n",
    "                        if faulty_io == 'EMPTY': return None\n",
    "                # if it still hasn't passed throw the error\n",
    "                return method(*args,**kwargs)\n",
    "            return wrap_method\n",
    "    \n",
    "    @staticmethod\n",
    "    def readBinaryBlobAsImageArray(iblob):\n",
    "        return imread(BytesIO(iblob))\n",
    "    \n",
    "    @staticmethod\n",
    "    def readBinaryBlobAsDicomArray(iblob):\n",
    "        sio_blob = BytesIO(iblob)\n",
    "        return read_file(sio_blob)\n",
    "    \n",
    "    @staticmethod\n",
    "    def imageTableToDataFrame(imt_rdd):\n",
    "        return imt_rdd.map(lambda x: dict(list(x[0].iteritems())+[('image_data',x[1].tolist())])).toDF()\n",
    "    \n",
    "    \n",
    "    def readImageDirectory(self, path, parts = 100):\n",
    "        \"\"\"\n",
    "        Read a directory of images\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        path : String\n",
    "            A path with wildcards for the images files can be prefixed with (s3, s3a, or a shared directory)\n",
    "        \"\"\"\n",
    "        read_fun = PyqaeContext._wrapIOCalls(PyqaeContext.readBinaryBlobAsImageArray, self.faulty_io, self.retry_att)\n",
    "        return self._cur_sc.binaryFiles(path, parts).mapValues(read_fun)\n",
    "    \n",
    "    def readDicomDirectory(self, path, parts = 100):\n",
    "        \"\"\"\n",
    "        Read a directory of dicom files\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        path : String\n",
    "            A path with wildcards for the images files can be prefixed with (s3, s3a, or a shared directory)\n",
    "        \"\"\"\n",
    "        read_fun = PyqaeContext._wrapIOCalls(PyqaeContext.readBinaryBlobAsDicomArray, self.faulty_io, self.retry_att)\n",
    "        return self._cur_sc.binaryFiles(path, parts).mapValues(read_fun)\n",
    "    \n",
    "    def readImageTable(self, path, col_name, im_path_prefix = '', n_partitions = 100, \n",
    "                       read_table_func = pd.read_csv, preproc_func = None):\n",
    "        \"\"\"\n",
    "        Read a table from images from a csv file\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        path : String\n",
    "            A path to the csv file\n",
    "        n_partitions: Int\n",
    "            The number of partitions to have\n",
    "        col_name : String\n",
    "            The name of the column containing the path to individual images\n",
    "        im_path_prefix : String\n",
    "            The prefix to append to the path in the text file so it is opened correctly (default empty)\n",
    "        read_table_func: Function (String -> Pandas DataFrame)\n",
    "            The function to read the table from a file-buffer object (default is the read_csv function)\n",
    "        preproc_func: Function (ndarray -> ndarray)\n",
    "            A function to preprocess the image (filtering, resizing, padding, etc)\n",
    "        \"\"\"\n",
    "        c_file = self._cur_sc.wholeTextFiles(path,1)\n",
    "        assert c_file.count()==1, \"This function only support a single file at the moment\"\n",
    "        full_table_buffer = StringIO(\"\\n\".join(c_file.map(lambda x: x[1]).collect()))\n",
    "        image_table = read_table_func(full_table_buffer)\n",
    "        image_paths = [os.path.join(im_path_prefix,cpath) for cpath in image_table[col_name]]\n",
    "        # read the binary files from a list\n",
    "        rawimg_rdd = self._cur_sc.binaryFiles(\",\".join(image_paths),n_partitions)\n",
    "        read_fun = PyqaeContext._wrapIOCalls(PyqaeContext.readBinaryBlobAsImageArray, self.faulty_io, self.retry_att)\n",
    "        img_rdd = rawimg_rdd.mapValues(read_fun)\n",
    "        pp_img_rdd = img_rdd if preproc_func is None else img_rdd.mapValues(preproc_func)\n",
    "        # add the file prefix so the keys come up in the map operation\n",
    "        image_paths = ['file:{}'.format(cpath) if cpath.find(':')<0 else cpath for cpath in image_paths]\n",
    "        image_list = dict(zip(image_paths,image_table.T.to_dict().values()))\n",
    "        \n",
    "        return img_rdd.map(lambda x: (image_list[x[0]],x[1]))\n",
    "    \n",
    "    @staticmethod\n",
    "    def _imageTableToDataFrame(imd_rdd, cur_sql, img_key_name = \"image\"):\n",
    "        \"\"\"\n",
    "        Converts an image table to a DataFrame by converting the ndarray into a nested list (inefficient)\n",
    "        but necessary for JVM compatibility. Written as a staticmethod to encapsulate the serialization.\n",
    "        #TODO implement ndarray <-> JVM exchange\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        imd_rdd: RDD[(dict[String,_], ndarray)]\n",
    "            The imageTable (created by readImageTable)\n",
    "        \n",
    "        cur_sql: SQLContext\n",
    "            The SQLContext in which to make the DataFrame (important for making tables later)\n",
    "        \n",
    "        \"\"\"\n",
    "        first_row_dict, _ = imd_rdd.first()\n",
    "        im_tbl_keys = list(first_row_dict.keys())\n",
    "        #TODO handle key missing errors more gracefully\n",
    "        iml_rdd = imd_rdd.map(lambda kv_pair: [kv_pair[0].get(ikey) for ikey in im_tbl_keys]+[kv_pair[1].tolist()])\n",
    "        return cur_sql.createDataFrame(iml_rdd, im_tbl_keys+[img_key_name])\n",
    "    \n",
    "    def readImageDataFrame(self, path, col_name, im_path_prefix = '', n_partitions = 100, \n",
    "                           read_table_func = pd.read_csv, preproc_func = None,\n",
    "                          sqlContext = None):\n",
    "        \"\"\"\n",
    "        Read a table from images from a csv file and return as a dataframe\n",
    "        See Help from [[readImageTable]]\n",
    "        Parameters\n",
    "        ----------\n",
    "        \n",
    "        sqlContext: SQLContext\n",
    "            The SQL context to use (if one exists) otherwise make a new one\n",
    "        \"\"\"\n",
    "        imd_rdd = self.readImageTable(path, col_name, im_path_prefix = im_path_prefix, \n",
    "                                   n_partitions = n_partitions, read_table_func = read_table_func)\n",
    "        cur_sql = sqlContext if sqlContext is not None else SQLContext(self._cur_sc)\n",
    "        return PyqaeContext._imageTableToDataFrame(imd_rdd, cur_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('file:/Volumes/WinDisk/openi_images/00000-CXR1005.png', (420, 512, 3))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pq_context = PyqaeContext(sc)\n",
    "im_files = pq_context.readImageDirectory('/Volumes/WinDisk/openi_images/*.png')\n",
    "im_files.mapValues(lambda x: x.shape).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'Unnamed: 0': 0,\n",
       "  'abstract': '<p><b>Comparison: </b>None.</p><p><b>Indication: </b>Pruritic.</p><p><b>Findings: </b>Cardiac and mediastinal contours are within normal limits. The lungs are clear. Bony structures are intact.</p><p><b>Impression: </b>No acute findings.</p>',\n",
       "  'caption': 'Chest, 2 views, frontal and lateral',\n",
       "  'image_id': 'F1',\n",
       "  'local_path': 'openi_images/00000-CXR1005.png',\n",
       "  'major': 'normal',\n",
       "  'minor': nan,\n",
       "  'problem': 'normal',\n",
       "  'row': 0,\n",
       "  'uid': 'CXR1005',\n",
       "  'url': '/imgs/512/203/1005/CXR1005_IM-0006-1001.png'},\n",
       " (420, 512, 3))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pq_context2 = PyqaeContext(sc)\n",
    "dim_files = pq_context2.readImageTable('/Volumes/WinDisk/openi_db_path.csv',\n",
    "                         'local_path', im_path_prefix = '/Volumes/WinDisk/')\n",
    "dim_files.mapValues(lambda x: x.shape).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[minor: double, Unnamed: 0: bigint, url: string, uid: string, major: string, image_id: string, caption: string, problem: string, abstract: string, local_path: string, row: bigint, image: array<array<array<bigint>>>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pq_context2 = PyqaeContext(sc)\n",
    "df_files = pq_context2.readImageDataFrame('/Volumes/WinDisk/openi_db_path.csv',\n",
    "                         'local_path', im_path_prefix = '/Volumes/WinDisk/', \n",
    "                                          n_partitions = 2000,\n",
    "                                          sqlContext = sqlContext)\n",
    "f_row = df_files.first()\n",
    "\n",
    "df_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o121.parquet.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1.apply$mcV$sp(InsertIntoHadoopFsRelationCommand.scala:149)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1.apply(InsertIntoHadoopFsRelationCommand.scala:115)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1.apply(InsertIntoHadoopFsRelationCommand.scala:115)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:115)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:60)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:58)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:136)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:114)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:86)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:86)\n\tat org.apache.spark.sql.execution.datasources.DataSource.write(DataSource.scala:487)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:211)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:194)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:478)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:128)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:211)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.SparkException: Job 9 cancelled because Stage 9 was cancelled\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1389)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleStageCancellation$1.apply$mcVI$sp(DAGScheduler.scala:1377)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleStageCancellation$1.apply(DAGScheduler.scala:1376)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleStageCancellation$1.apply(DAGScheduler.scala:1376)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofInt.foreach(ArrayOps.scala:234)\n\tat org.apache.spark.scheduler.DAGScheduler.handleStageCancellation(DAGScheduler.scala:1376)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1632)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1871)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1884)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1904)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1.apply$mcV$sp(InsertIntoHadoopFsRelationCommand.scala:143)\n\t... 30 more\n",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-bedd9276f744>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# save the table as parquet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_files\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumnRenamed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unnamed: 0'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/Volumes/WinDisk/full_open_db.pqt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Volumes/ExDisk/spark-2.0.0-bin-hadoop2.7/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m    615\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_opts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 617\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Volumes/ExDisk/spark-2.0.0-bin-hadoop2.7/python/lib/py4j-0.10.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m         return_value = get_return_value(\n\u001b[0;32m--> 933\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m    934\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Volumes/ExDisk/spark-2.0.0-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Volumes/ExDisk/spark-2.0.0-bin-hadoop2.7/python/lib/py4j-0.10.1-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    310\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    311\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    313\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o121.parquet.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1.apply$mcV$sp(InsertIntoHadoopFsRelationCommand.scala:149)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1.apply(InsertIntoHadoopFsRelationCommand.scala:115)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1.apply(InsertIntoHadoopFsRelationCommand.scala:115)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:115)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:60)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:58)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:136)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:114)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:86)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:86)\n\tat org.apache.spark.sql.execution.datasources.DataSource.write(DataSource.scala:487)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:211)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:194)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:478)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:128)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:211)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.SparkException: Job 9 cancelled because Stage 9 was cancelled\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1389)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleStageCancellation$1.apply$mcVI$sp(DAGScheduler.scala:1377)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleStageCancellation$1.apply(DAGScheduler.scala:1376)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleStageCancellation$1.apply(DAGScheduler.scala:1376)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofInt.foreach(ArrayOps.scala:234)\n\tat org.apache.spark.scheduler.DAGScheduler.handleStageCancellation(DAGScheduler.scala:1376)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1632)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1871)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1884)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1904)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1.apply$mcV$sp(InsertIntoHadoopFsRelationCommand.scala:143)\n\t... 30 more\n"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# save the table as parquet\n",
    "df_files.withColumnRenamed('Unnamed: 0','id').sample(False, 0.1).write.parquet(\"/Volumes/WinDisk/full_open_db.pqt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save the table as parquet\n",
    "df_files.withColumnRenamed('Unnamed: 0','id').write.parquet(\"/Volumes/WinDisk/full_open_db.pqt\")\n",
    "# save the table as parquet\n",
    "df_files.withColumnRenamed('Unnamed: 0','id').sample(False, 0.1).write.parquet(\"/Volumes/WinDisk/small_open_db.pqt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "cf_file = glob('/Users/mader/Dropbox/4Quant/Projects/TumorSegmentation/10092825/0013_t1_tse_tra_+c/*.dcm')[0]\n",
    "with open(cf_file,'r') as ifile:\n",
    "    sdata = StringIO(ifile.read())\n",
    "\n",
    "read_file(BytesIO(sdata.readlines()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "sc.binaryFiles('/Users/mader/Dropbox/4Quant/Projects/TumorSegmentation/10092825/0002_t2_blade_tra/*.dcm').mapValues(lambda x: read_file(BytesIO(x))).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pq_context3 = PyqaeContext(sc)\n",
    "dcm_files = pq_context3.readDicomDirectory('/Users/mader/Dropbox/4Quant/Projects/TumorSegmentation/10092825/0002_t2_blade_tra/*.dcm')\n",
    "dcm_files.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dim_files.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d_table = pq_context2.imageTableToDataFrame(dim_files)\n",
    "d_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for cpath, c_record in zip(test_table.head()['local_path'],test_table.head().to_records()):\n",
    "    print(cpath,c_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "?sc.binaryFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "?sc.binaryRecords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "?requests.get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a=np.zeros((3,3,3))\n",
    "import urllib\n",
    "urllib.parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import numpy as np\n",
    "try:\n",
    "    import urlparse\n",
    "except: # for python 3\n",
    "    from urllib import parse as urlparse\n",
    "class DRESTAccess(object):\n",
    "    \"\"\"\n",
    "    A distributed access to a REST interface\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_url, fetch_path, def_args, verbose = False):\n",
    "        self.base_url = base_url\n",
    "        self.fetch_path = fetch_path\n",
    "        self.def_args = def_args\n",
    "        self.verbose = verbose\n",
    "        \n",
    "    @staticmethod\n",
    "    def jsonRequest(req_url, args):\n",
    "        response = requests.get(req_url, args)\n",
    "        if response.ok:\n",
    "            return json.loads(response.content)\n",
    "        raise ValueError(\"{} could not be processed correctly\".format(req_url),args)\n",
    "    \n",
    "    @staticmethod\n",
    "    def bufferRequest(req_url, args):\n",
    "        response = requests.get(req_url, args)\n",
    "        if response.ok:\n",
    "            return StringIO(response.content)\n",
    "        raise ValueError(\"{} could not be processed correctly\".format(req_url),args)\n",
    "    \n",
    "    def pull_results(self, **args):\n",
    "        full_url = urlparse.urljoin(self.base_url,self.fetch_path)\n",
    "        new_param = dict(self.def_args + list(args.iteritems()))\n",
    "        print(full_url, new_param)\n",
    "        return DRESTAccess.jsonRequest(full_url, new_param)\n",
    "    \n",
    "    def parallel_pull(self, sc, arg_list, parts = 10):\n",
    "        return sc.parallelize(arg_list).map(lambda x: self.pull_results(**x))\n",
    "    \n",
    "\n",
    "class OpenIDB(DRESTAccess):\n",
    "    def __init__(self, step_count = 50):\n",
    "        self.step_count = step_count\n",
    "        DRESTAccess.__init__(self,\n",
    "                         base_url = \"https://openi.nlm.nih.gov\", \n",
    "                        fetch_path = \"retrieve.php\",\n",
    "                        def_args = [])\n",
    "    \n",
    "    def db_query(self, sc, **args):\n",
    "        base_args = list(args.iteritems())\n",
    "        test_query = self.pull_results(**dict(base_args + [('m',1), ('n',1)]))\n",
    "        m_range = np.arange(1,test_query['total'],self.step_count)\n",
    "        n_range = np.append(m_range[1:],test_query['total'])\n",
    "        qry_rdd = self.parallel_pull(sc, [dict(base_args + [('m',m), ('n',n)]) for m,n in zip(m_range,n_range)])\n",
    "        return qry_rdd.flatMap(lambda x: x['list'])\n",
    "    \n",
    "    @staticmethod\n",
    "    def format_entry(ie):\n",
    "        return {\n",
    "            'uid': ie['uid'],\n",
    "            'major': \";\".join(ie['MeSH']['major']), \n",
    "               'minor': \";\".join(ie['MeSH']['minor']), \n",
    "               'problem': ie['Problems'],\n",
    "              'abstract':ie['abstract'],\n",
    "               'caption':ie['image']['caption'],\n",
    "               'image_id':ie['image']['id'],\n",
    "            'url': ie['imgLarge']\n",
    "              }\n",
    "    \n",
    "    def get_collection(self,sc, coll='cxr', it='xg', lic='byncnd', **args):\n",
    "        \"\"\"\n",
    "        Fetch an entire collection of images as a dataframe\n",
    "        \"\"\"\n",
    "        study_results = self.db_query(sc, **dict(list(args.iteritems())+\n",
    "                                                 [('coll',coll), ('it',it), ('lic', lic)]))\n",
    "        return study_results.map(OpenIDB.format_entry).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "odb = OpenIDB()\n",
    "#odb.pull_results(m=1, n=1, coll='cxr', it='xg', lic='byncnd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_results = odb.db_query(sc, coll='cxr', it='xg', lic='byncnd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_results.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nw_results = odb.get_collection(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nw_results.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nw_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nw_results.registerTempTable(\"LungStudy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sqlContext.sql(\"Hey\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}