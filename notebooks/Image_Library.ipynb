{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from skimage.io import imread\n",
    "import pandas as pd\n",
    "from StringIO import StringIO\n",
    "from dicom import read_file\n",
    "class PyqaeContext(object):\n",
    "    \"\"\"\n",
    "    The primary context for performing PYQAE functions\n",
    "    \"\"\"\n",
    "    def __init__(self, cur_sc = None, faulty_io = 'FAIL', retry_att = 5, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Create or initialize a new Pyqae Context\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        cur_sc : SparkContext\n",
    "            An existing initialized SparkContext, if none a new one is initialized with the other parameters.\n",
    "        faulty_io : String\n",
    "            A string indicating what should happen if a file is missing (FAIL, RETRY, or return an EMPTY value)\n",
    "        retry_att : Int\n",
    "            The number of times a retry should be attempted (if faulty_io is in mode RETRY otherwise ignored)\n",
    "        \"\"\"\n",
    "        assert faulty_io in ['FAIL', 'RETRY', 'EMPTY'], \"Faulty IO must be in the list of FAIL, RETRY, or EMPTY\"\n",
    "        assert retry_att>0, \"Retry attempt must be greater than 0\"\n",
    "        self.faulty_io = faulty_io\n",
    "        self.retry_att = retry_att\n",
    "        if cur_sc is None: \n",
    "            from pyspark import SparkContext\n",
    "            self._cur_sc = SparkContext(*args, **kwargs)\n",
    "        else:\n",
    "            self._cur_sc = cur_sc\n",
    "    \n",
    "    @staticmethod\n",
    "    def _wrapIOCalls(method, faulty_io, retry_att):\n",
    "        \"\"\"\n",
    "        A general wrapper for IO calls which should be retried or returned empty \n",
    "        \n",
    "        \"\"\"\n",
    "        assert faulty_io in ['FAIL', 'RETRY', 'EMPTY']\n",
    "        assert retry_att > 0, \"Retry attempts should be more than 0, {}\".format(retry_att)\n",
    "        if faulty_io == 'FAIL':\n",
    "            return method\n",
    "        else:\n",
    "            def wrap_method(*args, **kwargs):\n",
    "                if faulty_io == 'RETRY': max_iter = retry_att-1\n",
    "                else: max_iter = 1\n",
    "                \n",
    "                for i in range(max_iter):\n",
    "                    try:\n",
    "                        return method(*args,**kwargs)\n",
    "                    except:\n",
    "                        if faulty_io == 'EMPTY': return None\n",
    "                # if it still hasn't passed throw the error\n",
    "                return method(*args,**kwargs)\n",
    "            return wrap_method\n",
    "    \n",
    "    @staticmethod\n",
    "    def readBinaryBlobAsImageArray(iblob):\n",
    "        return imread(StringIO(iblob))\n",
    "    \n",
    "    @staticmethod\n",
    "    def readBinaryBlobAsDicomArray(iblob):\n",
    "        sio_blob = BytesIO(iblob)\n",
    "        return read_file(sio_blob)\n",
    "    \n",
    "    @staticmethod\n",
    "    def imageTableToDataFrame(imt_rdd):\n",
    "        return imt_rdd.map(lambda x: dict(list(x[0].iteritems())+[('image_data',x[1].tolist())])).toDF()\n",
    "    \n",
    "    \n",
    "    def readImageDirectory(self, path, parts = 100):\n",
    "        \"\"\"\n",
    "        Read a directory of images\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        path : String\n",
    "            A path with wildcards for the images files can be prefixed with (s3, s3a, or a shared directory)\n",
    "        \"\"\"\n",
    "        read_fun = PyqaeContext._wrapIOCalls(PyqaeContext.readBinaryBlobAsImageArray, self.faulty_io, self.retry_att)\n",
    "        return self._cur_sc.binaryFiles(path, parts).mapValues(read_fun)\n",
    "    \n",
    "    def readDicomDirectory(self, path, parts = 100):\n",
    "        \"\"\"\n",
    "        Read a directory of dicom files\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        path : String\n",
    "            A path with wildcards for the images files can be prefixed with (s3, s3a, or a shared directory)\n",
    "        \"\"\"\n",
    "        read_fun = PyqaeContext._wrapIOCalls(PyqaeContext.readBinaryBlobAsDicomArray, self.faulty_io, self.retry_att)\n",
    "        return self._cur_sc.binaryFiles(path, parts).mapValues(read_fun)\n",
    "    \n",
    "    def readImageTable(self, path, col_name, im_path_prefix = '', parts = 100, read_table_func = pd.read_csv):\n",
    "        \"\"\"\n",
    "        Read a table from images from a csv file\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        path : String\n",
    "            A path to the csv file\n",
    "        col_name : String\n",
    "            The name of the column containing the path to individual images\n",
    "        im_path_prefix : String\n",
    "            The prefix to append to the path in the text file so it is opened correctly (default empty)\n",
    "        read_table_func: Function (String -> Pandas DataFrame)\n",
    "            The function to read the table from a file-buffer object (default is the read_csv function)\n",
    "        \"\"\"\n",
    "        c_file = self._cur_sc.wholeTextFiles(path,1)\n",
    "        assert c_file.count()==1, \"This function only support a single file at the moment\"\n",
    "        full_table_buffer = StringIO(\"\\n\".join(c_file.map(lambda x: x[1]).collect()))\n",
    "        image_table = read_table_func(full_table_buffer)\n",
    "        image_paths = [os.path.join(im_path_prefix,cpath) for cpath in image_table[col_name]]\n",
    "        \n",
    "        rawimg_rdd = self._cur_sc.binaryFiles(\",\".join(image_paths),parts)\n",
    "        read_fun = PyqaeContext._wrapIOCalls(PyqaeContext.readBinaryBlobAsImageArray, self.faulty_io, self.retry_att)\n",
    "        img_rdd = rawimg_rdd.mapValues(read_fun)\n",
    "        # add the file prefix so the keys come up in the map operation\n",
    "        image_paths = ['file:{}'.format(cpath) if cpath.find(':')<0 else cpath for cpath in image_paths]\n",
    "        image_list = dict(zip(image_paths,image_table.T.to_dict().values()))\n",
    "        \n",
    "        return img_rdd.map(lambda x: (image_list[x[0]],x[1]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(u'file:/Users/mader/Dropbox/4Quant/Projects/PACScrawlertools/openi_images/00000-CXR1005.png',\n",
       " (420, 512, 3))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pq_context = PyqaeContext(sc)\n",
    "im_files = pq_context.readImageDirectory('/Users/mader/Dropbox/4Quant/Projects/PACScrawlertools/openi_images/*.png')\n",
    "im_files.mapValues(lambda x: x.shape).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0008, 0008) Image Type                          CS: ['ORIGINAL', 'PRIMARY', 'M', 'NORM', 'DIS2D', 'FM', 'FIL']\n",
       "(0008, 0012) Instance Creation Date              DA: '20140606'\n",
       "(0008, 0013) Instance Creation Time              TM: '114502.640000'\n",
       "(0008, 0016) SOP Class UID                       UI: MR Image Storage\n",
       "(0008, 0018) SOP Instance UID                    UI: 1.3.12.2.1107.5.2.32.35424.2014060611450240186479552\n",
       "(0008, 0020) Study Date                          DA: '20140606'\n",
       "(0008, 0021) Series Date                         DA: '20140606'\n",
       "(0008, 0022) Acquisition Date                    DA: '20140606'\n",
       "(0008, 0023) Content Date                        DA: '20140606'\n",
       "(0008, 0030) Study Time                          TM: '111723.734000'\n",
       "(0008, 0031) Series Time                         TM: '114502.562000'\n",
       "(0008, 0032) Acquisition Time                    TM: '114327.532500'\n",
       "(0008, 0033) Content Time                        TM: '114502.640000'\n",
       "(0008, 0050) Accession Number                    SH: 'ME140606MR3016'\n",
       "(0008, 0060) Modality                            CS: 'MR'\n",
       "(0008, 0070) Manufacturer                        LO: 'SIEMENS'\n",
       "(0008, 0080) Institution Name                    LO: 'SYSU Cancer Center'\n",
       "(0008, 0081) Institution Address                 ST: 'Street StreetNo,Guangzhou//FA4C24//,District,CN,ZIP'\n",
       "(0008, 0090) Referring Physician's Name          PN: ''\n",
       "(0008, 1010) Station Name                        SH: 'MRC35424'\n",
       "(0008, 1030) Study Description                   LO: 'peng^general'\n",
       "(0008, 1032)  Procedure Code Sequence   1 item(s) ---- \n",
       "\n",
       "   ---------\n",
       "(0008, 103e) Series Description                  LO: 't1_tse_tra_+c'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from glob import glob\n",
    "cf_file = glob('/Users/mader/Dropbox/4Quant/Projects/TumorSegmentation/10092825/0013_t1_tse_tra_+c/*.dcm')[0]\n",
    "with open(cf_file,'r') as ifile:\n",
    "    sdata = StringIO(ifile.read())\n",
    "\n",
    "read_file(BytesIO(sdata.readlines()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Dataset contents must be DataElement instances.\nTo set a data_element value use data_element.value=val",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-8771b9167ad1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mio\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinaryFiles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/Users/mader/Dropbox/4Quant/Projects/TumorSegmentation/10092825/0002_t2_blade_tra/*.dcm'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mread_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Volumes/ExDisk/spark-2.0.0-bin-hadoop2.7/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36mfirst\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1326\u001b[0m         \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRDD\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mempty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m         \"\"\"\n\u001b[0;32m-> 1328\u001b[0;31m         \u001b[0mrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1329\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Volumes/ExDisk/spark-2.0.0-bin-hadoop2.7/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1310\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1312\u001b[0m             \u001b[0mitems\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Volumes/ExDisk/spark-2.0.0-bin-hadoop2.7/python/pyspark/context.pyc\u001b[0m in \u001b[0;36mrunJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m    940\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m         \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    943\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mshow_profiles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Volumes/ExDisk/spark-2.0.0-bin-hadoop2.7/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36m_load_from_socket\u001b[0;34m(port, serializer)\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0mrf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakefile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m65536\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mserializer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Volumes/ExDisk/spark-2.0.0-bin-hadoop2.7/python/pyspark/serializers.pyc\u001b[0m in \u001b[0;36mload_stream\u001b[0;34m(self, stream)\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_with_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mEOFError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Volumes/ExDisk/spark-2.0.0-bin-hadoop2.7/python/pyspark/serializers.pyc\u001b[0m in \u001b[0;36m_read_with_length\u001b[0;34m(self, stream)\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mEOFError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Volumes/ExDisk/spark-2.0.0-bin-hadoop2.7/python/pyspark/serializers.pyc\u001b[0m in \u001b[0;36mloads\u001b[0;34m(self, obj, encoding)\u001b[0m\n\u001b[1;32m    420\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mader/anaconda/lib/python2.7/site-packages/dicom/dataset.pyc\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m    516\u001b[0m         \u001b[0;34m\"\"\"Operator for dataset[key]=value. Check consistency, and deal with private tags\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mDataElement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRawDataElement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# ok if is subclass, e.g. DeferredDataElement\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 518\u001b[0;31m             raise TypeError(\"Dataset contents must be DataElement instances.\\n\"\n\u001b[0m\u001b[1;32m    519\u001b[0m                             \"To set a data_element value use data_element.value=val\")\n\u001b[1;32m    520\u001b[0m         \u001b[0mtag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Dataset contents must be DataElement instances.\nTo set a data_element value use data_element.value=val"
     ]
    }
   ],
   "source": [
    "from io import BytesIO\n",
    "sc.binaryFiles('/Users/mader/Dropbox/4Quant/Projects/TumorSegmentation/10092825/0002_t2_blade_tra/*.dcm').mapValues(lambda x: read_file(BytesIO(x))).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Dataset contents must be DataElement instances.\nTo set a data_element value use data_element.value=val",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-4183b3f1de88>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpq_context3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPyqaeContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdcm_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpq_context3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadDicomDirectory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/Users/mader/Dropbox/4Quant/Projects/TumorSegmentation/10092825/0002_t2_blade_tra/*.dcm'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdcm_files\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Volumes/ExDisk/spark-2.0.0-bin-hadoop2.7/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1310\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1312\u001b[0m             \u001b[0mitems\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Volumes/ExDisk/spark-2.0.0-bin-hadoop2.7/python/pyspark/context.pyc\u001b[0m in \u001b[0;36mrunJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m    940\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m         \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    943\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mshow_profiles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Volumes/ExDisk/spark-2.0.0-bin-hadoop2.7/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36m_load_from_socket\u001b[0;34m(port, serializer)\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0mrf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakefile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m65536\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mserializer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Volumes/ExDisk/spark-2.0.0-bin-hadoop2.7/python/pyspark/serializers.pyc\u001b[0m in \u001b[0;36mload_stream\u001b[0;34m(self, stream)\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_with_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mEOFError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Volumes/ExDisk/spark-2.0.0-bin-hadoop2.7/python/pyspark/serializers.pyc\u001b[0m in \u001b[0;36m_read_with_length\u001b[0;34m(self, stream)\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mEOFError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Volumes/ExDisk/spark-2.0.0-bin-hadoop2.7/python/pyspark/serializers.pyc\u001b[0m in \u001b[0;36mloads\u001b[0;34m(self, obj, encoding)\u001b[0m\n\u001b[1;32m    420\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mader/anaconda/lib/python2.7/site-packages/dicom/dataset.pyc\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m    516\u001b[0m         \u001b[0;34m\"\"\"Operator for dataset[key]=value. Check consistency, and deal with private tags\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mDataElement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRawDataElement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# ok if is subclass, e.g. DeferredDataElement\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 518\u001b[0;31m             raise TypeError(\"Dataset contents must be DataElement instances.\\n\"\n\u001b[0m\u001b[1;32m    519\u001b[0m                             \"To set a data_element value use data_element.value=val\")\n\u001b[1;32m    520\u001b[0m         \u001b[0mtag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Dataset contents must be DataElement instances.\nTo set a data_element value use data_element.value=val"
     ]
    }
   ],
   "source": [
    "pq_context3 = PyqaeContext(sc)\n",
    "dcm_files = pq_context3.readDicomDirectory('/Users/mader/Dropbox/4Quant/Projects/TumorSegmentation/10092825/0002_t2_blade_tra/*.dcm')\n",
    "dcm_files.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'Unnamed: 0': 0,\n",
       "  'abstract': '<p><b>Comparison: </b>None.</p><p><b>Indication: </b>Pruritic.</p><p><b>Findings: </b>Cardiac and mediastinal contours are within normal limits. The lungs are clear. Bony structures are intact.</p><p><b>Impression: </b>No acute findings.</p>',\n",
       "  'caption': 'Chest, 2 views, frontal and lateral',\n",
       "  'image_id': 'F1',\n",
       "  'local_path': 'openi_images/00000-CXR1005.png',\n",
       "  'major': 'normal',\n",
       "  'minor': nan,\n",
       "  'problem': 'normal',\n",
       "  'row': 0,\n",
       "  'uid': 'CXR1005',\n",
       "  'url': '/imgs/512/203/1005/CXR1005_IM-0006-1001.png'},\n",
       " (420, 512, 3))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pq_context2 = PyqaeContext(sc)\n",
    "dim_files = pq_context2.readImageTable('/Users/mader/Dropbox/4Quant/Projects/PACScrawlertools/openi_db_path.csv',\n",
    "                         'local_path',\n",
    "                         im_path_prefix = '/Users/mader/Dropbox/4Quant/Projects/PACScrawlertools/')\n",
    "dim_files.mapValues(lambda x: x.shape).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7619"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim_files.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Unnamed: 0: bigint, abstract: string, caption: string, image_data: array<array<array<bigint>>>, image_id: string, local_path: string, major: string, minor: double, problem: string, row: bigint, uid: string, url: string]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_table = pq_context2.imageTableToDataFrame(dim_files)\n",
    "d_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('openi_images/00000-CXR1005.png', (0, 0, '<p><b>Comparison: </b>None.</p><p><b>Indication: </b>Pruritic.</p><p><b>Findings: </b>Cardiac and mediastinal contours are within normal limits. The lungs are clear. Bony structures are intact.</p><p><b>Impression: </b>No acute findings.</p>', 'Chest, 2 views, frontal and lateral', 'F1', 'normal', nan, 'normal', 'CXR1005', '/imgs/512/203/1005/CXR1005_IM-0006-1001.png', 0, 'openi_images/00000-CXR1005.png'))\n",
      "('openi_images/00001-CXR1005.png', (1, 1, '<p><b>Comparison: </b>None.</p><p><b>Indication: </b>Pruritic.</p><p><b>Findings: </b>Cardiac and mediastinal contours are within normal limits. The lungs are clear. Bony structures are intact.</p><p><b>Impression: </b>No acute findings.</p>', 'Chest, 2 views, frontal and lateral', 'F2', 'normal', nan, 'normal', 'CXR1005', '/imgs/512/203/1005/CXR1005_IM-0006-3003.png', 1, 'openi_images/00001-CXR1005.png'))\n",
      "('openi_images/00002-CXR1002.png', (2, 2, '<p><b>Indication: </b>History of chest pain</p><p><b>Impression: </b>Status post left mastectomy. Heart size normal. Lungs are clear.</p>', 'PA and lateral chest XXXX, XXXX XXXX comparison XXXX XXXX ', 'F1', 'Mastectomy/left', nan, 'Mastectomy', 'CXR1002', '/imgs/512/200/1002/CXR1002_IM-0004-1001.png', 2, 'openi_images/00002-CXR1002.png'))\n",
      "('openi_images/00003-CXR1002.png', (3, 3, '<p><b>Indication: </b>History of chest pain</p><p><b>Impression: </b>Status post left mastectomy. Heart size normal. Lungs are clear.</p>', 'PA and lateral chest XXXX, XXXX XXXX comparison XXXX XXXX ', 'F2', 'Mastectomy/left', nan, 'Mastectomy', 'CXR1002', '/imgs/512/200/1002/CXR1002_IM-0004-2001.png', 3, 'openi_images/00003-CXR1002.png'))\n",
      "('openi_images/00004-CXR1.png', (4, 4, '<p><b>Comparison: </b>None.</p><p><b>Indication: </b>Positive TB test</p><p><b>Findings: </b>The cardiac silhouette and mediastinum size are within normal limits. There is no pulmonary edema. There is no focal consolidation. There are no XXXX of a pleural effusion. There is no evidence of pneumothorax.</p><p><b>Impression: </b>Normal chest x-XXXX.</p>', 'Xray Chest PA and Lateral', 'F1', 'normal', nan, 'normal', 'CXR1', '/imgs/512/1/1/CXR1_1_IM-0001-3001.png', 4, 'openi_images/00004-CXR1.png'))\n"
     ]
    }
   ],
   "source": [
    "for cpath, c_record in zip(test_table.head()['local_path'],test_table.head().to_records()):\n",
    "    print(cpath,c_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "?read_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "?sc.binaryFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "?sc.binaryRecords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "?requests.get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a=np.zeros((3,3,3))\n",
    "import urllib\n",
    "urllib.parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import numpy as np\n",
    "try:\n",
    "    import urlparse\n",
    "except: # for python 3\n",
    "    from urllib import parse as urlparse\n",
    "class DRESTAccess(object):\n",
    "    \"\"\"\n",
    "    A distributed access to a REST interface\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_url, fetch_path, def_args, verbose = False):\n",
    "        self.base_url = base_url\n",
    "        self.fetch_path = fetch_path\n",
    "        self.def_args = def_args\n",
    "        self.verbose = verbose\n",
    "        \n",
    "    @staticmethod\n",
    "    def jsonRequest(req_url, args):\n",
    "        response = requests.get(req_url, args)\n",
    "        if response.ok:\n",
    "            return json.loads(response.content)\n",
    "        raise ValueError(\"{} could not be processed correctly\".format(req_url),args)\n",
    "    \n",
    "    @staticmethod\n",
    "    def bufferRequest(req_url, args):\n",
    "        response = requests.get(req_url, args)\n",
    "        if response.ok:\n",
    "            return StringIO(response.content)\n",
    "        raise ValueError(\"{} could not be processed correctly\".format(req_url),args)\n",
    "    \n",
    "    def pull_results(self, **args):\n",
    "        full_url = urlparse.urljoin(self.base_url,self.fetch_path)\n",
    "        new_param = dict(self.def_args + list(args.iteritems()))\n",
    "        print(full_url, new_param)\n",
    "        return DRESTAccess.jsonRequest(full_url, new_param)\n",
    "    \n",
    "    def parallel_pull(self, sc, arg_list, parts = 10):\n",
    "        return sc.parallelize(arg_list).map(lambda x: self.pull_results(**x))\n",
    "    \n",
    "\n",
    "class OpenIDB(DRESTAccess):\n",
    "    def __init__(self, step_count = 50):\n",
    "        self.step_count = step_count\n",
    "        DRESTAccess.__init__(self,\n",
    "                         base_url = \"https://openi.nlm.nih.gov\", \n",
    "                        fetch_path = \"retrieve.php\",\n",
    "                        def_args = [])\n",
    "    \n",
    "    def db_query(self, sc, **args):\n",
    "        base_args = list(args.iteritems())\n",
    "        test_query = self.pull_results(**dict(base_args + [('m',1), ('n',1)]))\n",
    "        m_range = np.arange(1,test_query['total'],self.step_count)\n",
    "        n_range = np.append(m_range[1:],test_query['total'])\n",
    "        qry_rdd = self.parallel_pull(sc, [dict(base_args + [('m',m), ('n',n)]) for m,n in zip(m_range,n_range)])\n",
    "        return qry_rdd.flatMap(lambda x: x['list'])\n",
    "    \n",
    "    @staticmethod\n",
    "    def format_entry(ie):\n",
    "        return {\n",
    "            'uid': ie['uid'],\n",
    "            'major': \";\".join(ie['MeSH']['major']), \n",
    "               'minor': \";\".join(ie['MeSH']['minor']), \n",
    "               'problem': ie['Problems'],\n",
    "              'abstract':ie['abstract'],\n",
    "               'caption':ie['image']['caption'],\n",
    "               'image_id':ie['image']['id'],\n",
    "            'url': ie['imgLarge']\n",
    "              }\n",
    "    \n",
    "    def get_collection(self,sc, coll='cxr', it='xg', lic='byncnd', **args):\n",
    "        \"\"\"\n",
    "        Fetch an entire collection of images as a dataframe\n",
    "        \"\"\"\n",
    "        study_results = self.db_query(sc, **dict(list(args.iteritems())+\n",
    "                                                 [('coll',coll), ('it',it), ('lic', lic)]))\n",
    "        return study_results.map(OpenIDB.format_entry).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "odb = OpenIDB()\n",
    "#odb.pull_results(m=1, n=1, coll='cxr', it='xg', lic='byncnd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('https://openi.nlm.nih.gov/retrieve.php', {'coll': 'cxr', 'n': 1, 'm': 1, 'it': 'xg', 'lic': 'byncnd'})\n"
     ]
    }
   ],
   "source": [
    "all_results = odb.db_query(sc, coll='cxr', it='xg', lic='byncnd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'MeSH': {u'major': [u'normal'], u'minor': []},\n",
       " u'Outcome': [{u'#text': u'', u'@score': u'-0.102'}],\n",
       " u'Problems': u'normal',\n",
       " u'abstract': u'<p><b>Comparison: </b>None.</p><p><b>Indication: </b>Pruritic.</p><p><b>Findings: </b>Cardiac and mediastinal contours are within normal limits. The lungs are clear. Bony structures are intact.</p><p><b>Impression: </b>No acute findings.</p>',\n",
       " u'affiliate': u'Indiana University',\n",
       " u'authors': u'Kohli MD, Rosenman M',\n",
       " u'ccLicense': u'byncnd',\n",
       " u'detailedQueryURL': u'retrieve.php?img=CXR1005_IM-0006-1001&query=&it=xg&coll=cxr&lic=byncnd&req=4',\n",
       " u'docSource': u'CXR',\n",
       " u'fulltext_html_url': u'',\n",
       " u'getArticleFigures': u'retrieve.php?uid=CXR1005&req=5',\n",
       " u'image': {u'caption': u'Chest, 2 views, frontal and lateral',\n",
       "  u'id': u'F1',\n",
       "  u'mention': u'',\n",
       "  u'modalityMajor': u'x'},\n",
       " u'imgGrid150': u'/imgs/150/203/1005/CXR1005_IM-0006-1001.png',\n",
       " u'imgLarge': u'/imgs/512/203/1005/CXR1005_IM-0006-1001.png',\n",
       " u'imgThumb': u'/imgs/100/203/1005/CXR1005_IM-0006-1001.png',\n",
       " u'imgThumbLarge': u'/imgs/137/203/1005/CXR1005_IM-0006-1001.png',\n",
       " u'journal_abbr': u'',\n",
       " u'journal_date': {u'day': u'01', u'month': u'08', u'year': u'2013'},\n",
       " u'journal_title': u'',\n",
       " u'licenseType': u'open-access',\n",
       " u'licenseURL': u'http://creativecommons.org/licenses/by-nc-nd/4.0/',\n",
       " u'note': u'The data are drawn from multiple hospital systems.',\n",
       " u'pmcid': u'1005',\n",
       " u'similarInCollection': u'retrieve.php?simCollection=CXR1005_IM-0006-1001&query=&req=3&it=xg',\n",
       " u'similarInResults': u'retrieve.php?simResults=CXR1005_IM-0006-1001&query=&it=xg&coll=cxr&lic=byncnd&req=2',\n",
       " u'title': u'Indiana University Chest X-ray Collection',\n",
       " u'uid': u'CXR1005'}"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_results.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('https://openi.nlm.nih.gov/retrieve.php', {'coll': 'cxr', 'n': 1, 'm': 1, 'it': 'xg', 'lic': 'byncnd'})\n"
     ]
    }
   ],
   "source": [
    "nw_results = odb.get_collection(sc) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(abstract=u'<p><b>Comparison: </b>None.</p><p><b>Indication: </b>Pruritic.</p><p><b>Findings: </b>Cardiac and mediastinal contours are within normal limits. The lungs are clear. Bony structures are intact.</p><p><b>Impression: </b>No acute findings.</p>', caption=u'Chest, 2 views, frontal and lateral', image_id=u'F1', major=u'normal', minor=u'', problem=u'normal', uid=u'CXR1005', url=u'/imgs/512/203/1005/CXR1005_IM-0006-1001.png')"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nw_results.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(abstract=u'<p><b>Comparison: </b>None.</p><p><b>Indication: </b>Pruritic.</p><p><b>Findings: </b>Cardiac and mediastinal contours are within normal limits. The lungs are clear. Bony structures are intact.</p><p><b>Impression: </b>No acute findings.</p>', caption=u'Chest, 2 views, frontal and lateral', image_id=u'F1', major=u'normal', minor=u'', problem=u'normal', uid=u'CXR1005', url=u'/imgs/512/203/1005/CXR1005_IM-0006-1001.png')"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nw_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nw_results.registerTempTable(\"LungStudy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sqlContext.sql(\"Hey\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
