{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Library Tools\n",
    "\n",
    "For use with a Pyspark backend (or fakespark, not fully supported yet)\n",
    "\n",
    "Use of notebooks in spark requires appropriate environment variables to be set\n",
    "\n",
    "```bash\n",
    "PYSPARK_DRIVER_PYTHON=jupyter PYSPARK_DRIVER_PYTHON_OPTS=notebook PYSPARK_PYTHON=/Users/mader/anaconda/bin/python /Volumes/ExDisk/spark-2.0.0-bin-hadoop2.7/bin/pyspark\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-cc8ebfbbd581>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mio\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mStringIO\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSQLContext\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mdicom\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mread_file\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from skimage.io import imread\n",
    "import pandas as pd\n",
    "from io import BytesIO, StringIO\n",
    "from pyspark import SQLContext\n",
    "\n",
    "from dicom import read_file\n",
    "class PyqaeContext(object):\n",
    "    \"\"\"\n",
    "    The primary context for performing PYQAE functions\n",
    "    \"\"\"\n",
    "    def __init__(self, cur_sc = None, faulty_io = 'FAIL', retry_att = 5, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Create or initialize a new Pyqae Context\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        cur_sc : SparkContext\n",
    "            An existing initialized SparkContext, if none a new one is initialized with the other parameters.\n",
    "        faulty_io : String\n",
    "            A string indicating what should happen if a file is missing (FAIL, RETRY, or return an EMPTY value)\n",
    "        retry_att : Int\n",
    "            The number of times a retry should be attempted (if faulty_io is in mode RETRY otherwise ignored)\n",
    "        \"\"\"\n",
    "        assert faulty_io in ['FAIL', 'RETRY', 'EMPTY'], \"Faulty IO must be in the list of FAIL, RETRY, or EMPTY\"\n",
    "        assert retry_att>0, \"Retry attempt must be greater than 0\"\n",
    "        self.faulty_io = faulty_io\n",
    "        self.retry_att = retry_att\n",
    "        if cur_sc is None: \n",
    "            from pyspark import SparkContext\n",
    "            self._cur_sc = SparkContext(*args, **kwargs)\n",
    "        else:\n",
    "            self._cur_sc = cur_sc\n",
    "    \n",
    "    @staticmethod\n",
    "    def _wrapIOCalls(method, faulty_io, retry_att):\n",
    "        \"\"\"\n",
    "        A general wrapper for IO calls which should be retried or returned empty \n",
    "        \n",
    "        \"\"\"\n",
    "        assert faulty_io in ['FAIL', 'RETRY', 'EMPTY']\n",
    "        assert retry_att > 0, \"Retry attempts should be more than 0, {}\".format(retry_att)\n",
    "        if faulty_io == 'FAIL':\n",
    "            return method\n",
    "        else:\n",
    "            def wrap_method(*args, **kwargs):\n",
    "                if faulty_io == 'RETRY': max_iter = retry_att-1\n",
    "                else: max_iter = 1\n",
    "                \n",
    "                for i in range(max_iter):\n",
    "                    try:\n",
    "                        return method(*args,**kwargs)\n",
    "                    except:\n",
    "                        if faulty_io == 'EMPTY': return None\n",
    "                # if it still hasn't passed throw the error\n",
    "                return method(*args,**kwargs)\n",
    "            return wrap_method\n",
    "    \n",
    "    @staticmethod\n",
    "    def readBinaryBlobAsImageArray(iblob):\n",
    "        return imread(BytesIO(iblob))\n",
    "    \n",
    "    @staticmethod\n",
    "    def readBinaryBlobAsDicomArray(iblob):\n",
    "        sio_blob = BytesIO(iblob)\n",
    "        return read_file(sio_blob)\n",
    "    \n",
    "    @staticmethod\n",
    "    def imageTableToDataFrame(imt_rdd):\n",
    "        return imt_rdd.map(lambda x: dict(list(x[0].iteritems())+[('image_data',x[1].tolist())])).toDF()\n",
    "    \n",
    "    \n",
    "    def readImageDirectory(self, path, parts = 100):\n",
    "        \"\"\"\n",
    "        Read a directory of images\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        path : String\n",
    "            A path with wildcards for the images files can be prefixed with (s3, s3a, or a shared directory)\n",
    "        \"\"\"\n",
    "        read_fun = PyqaeContext._wrapIOCalls(PyqaeContext.readBinaryBlobAsImageArray, self.faulty_io, self.retry_att)\n",
    "        return self._cur_sc.binaryFiles(path, parts).mapValues(read_fun)\n",
    "    \n",
    "    def readDicomDirectory(self, path, parts = 100):\n",
    "        \"\"\"\n",
    "        Read a directory of dicom files\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        path : String\n",
    "            A path with wildcards for the images files can be prefixed with (s3, s3a, or a shared directory)\n",
    "        \"\"\"\n",
    "        read_fun = PyqaeContext._wrapIOCalls(PyqaeContext.readBinaryBlobAsDicomArray, self.faulty_io, self.retry_att)\n",
    "        return self._cur_sc.binaryFiles(path, parts).mapValues(read_fun)\n",
    "    \n",
    "    def readImageTable(self, path, col_name, im_path_prefix = '', n_partitions = 100, \n",
    "                       read_table_func = pd.read_csv, preproc_func = None):\n",
    "        \"\"\"\n",
    "        Read a table from images from a csv file\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        path : String\n",
    "            A path to the csv file\n",
    "        n_partitions: Int\n",
    "            The number of partitions to have\n",
    "        col_name : String\n",
    "            The name of the column containing the path to individual images\n",
    "        im_path_prefix : String\n",
    "            The prefix to append to the path in the text file so it is opened correctly (default empty)\n",
    "        read_table_func: Function (String -> Pandas DataFrame)\n",
    "            The function to read the table from a file-buffer object (default is the read_csv function)\n",
    "        preproc_func: Function (ndarray -> ndarray)\n",
    "            A function to preprocess the image (filtering, resizing, padding, etc)\n",
    "        \"\"\"\n",
    "        c_file = self._cur_sc.wholeTextFiles(path,1)\n",
    "        assert c_file.count()==1, \"This function only support a single file at the moment\"\n",
    "        full_table_buffer = StringIO(\"\\n\".join(c_file.map(lambda x: x[1]).collect()))\n",
    "        image_table = read_table_func(full_table_buffer)\n",
    "        image_paths = [os.path.join(im_path_prefix,cpath) for cpath in image_table[col_name]]\n",
    "        # read the binary files from a list\n",
    "        rawimg_rdd = self._cur_sc.binaryFiles(\",\".join(image_paths),n_partitions)\n",
    "        read_fun = PyqaeContext._wrapIOCalls(PyqaeContext.readBinaryBlobAsImageArray, self.faulty_io, self.retry_att)\n",
    "        img_rdd = rawimg_rdd.mapValues(read_fun)\n",
    "        pp_img_rdd = img_rdd if preproc_func is None else img_rdd.mapValues(preproc_func)\n",
    "        # add the file prefix so the keys come up in the map operation\n",
    "        image_paths = ['file:{}'.format(cpath) if cpath.find(':')<0 else cpath for cpath in image_paths]\n",
    "        image_list = dict(zip(image_paths,image_table.T.to_dict().values()))\n",
    "        \n",
    "        return img_rdd.map(lambda x: (image_list[x[0]],x[1]))\n",
    "    \n",
    "    @staticmethod\n",
    "    def _imageTableToDataFrame(imd_rdd, cur_sql, img_key_name = \"image\"):\n",
    "        \"\"\"\n",
    "        Converts an image table to a DataFrame by converting the ndarray into a nested list (inefficient)\n",
    "        but necessary for JVM compatibility. Written as a staticmethod to encapsulate the serialization.\n",
    "        #TODO implement ndarray <-> JVM exchange\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        imd_rdd: RDD[(dict[String,_], ndarray)]\n",
    "            The imageTable (created by readImageTable)\n",
    "        \n",
    "        cur_sql: SQLContext\n",
    "            The SQLContext in which to make the DataFrame (important for making tables later)\n",
    "        \n",
    "        \"\"\"\n",
    "        first_row_dict, _ = imd_rdd.first()\n",
    "        im_tbl_keys = list(first_row_dict.keys())\n",
    "        #TODO handle key missing errors more gracefully\n",
    "        iml_rdd = imd_rdd.map(lambda kv_pair: [kv_pair[0].get(ikey) for ikey in im_tbl_keys]+[kv_pair[1].tolist()])\n",
    "        return cur_sql.createDataFrame(iml_rdd, im_tbl_keys+[img_key_name])\n",
    "    \n",
    "    def readImageDataFrame(self, path, col_name, im_path_prefix = '', n_partitions = 100, \n",
    "                           read_table_func = pd.read_csv, preproc_func = None,\n",
    "                          sqlContext = None):\n",
    "        \"\"\"\n",
    "        Read a table from images from a csv file and return as a dataframe\n",
    "        See Help from [[readImageTable]]\n",
    "        Parameters\n",
    "        ----------\n",
    "        \n",
    "        sqlContext: SQLContext\n",
    "            The SQL context to use (if one exists) otherwise make a new one\n",
    "        \"\"\"\n",
    "        imd_rdd = self.readImageTable(path, col_name, im_path_prefix = im_path_prefix, \n",
    "                                   n_partitions = n_partitions, read_table_func = read_table_func)\n",
    "        cur_sql = sqlContext if sqlContext is not None else SQLContext(self._cur_sc)\n",
    "        return PyqaeContext._imageTableToDataFrame(imd_rdd, cur_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pq_context = PyqaeContext(sc)\n",
    "im_files = pq_context.readImageDirectory('/Volumes/WinDisk/openi_images/*.png')\n",
    "im_files.mapValues(lambda x: x.shape).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pq_context2 = PyqaeContext(sc)\n",
    "dim_files = pq_context2.readImageTable('/Volumes/WinDisk/openi_db_path.csv',\n",
    "                         'local_path', im_path_prefix = '/Volumes/WinDisk/')\n",
    "dim_files.mapValues(lambda x: x.shape).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pq_context2 = PyqaeContext(sc)\n",
    "df_files = pq_context2.readImageDataFrame('/Volumes/WinDisk/openi_db_path.csv',\n",
    "                         'local_path', im_path_prefix = '/Volumes/WinDisk/', \n",
    "                                          n_partitions = 2000,\n",
    "                                          sqlContext = sqlContext)\n",
    "f_row = df_files.first()\n",
    "\n",
    "df_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the table as parquet\n",
    "df_files.withColumnRenamed('Unnamed: 0','id').sample(False, 0.1).write.parquet(\"/Volumes/WinDisk/full_open_db.pqt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the table as parquet\n",
    "df_files.withColumnRenamed('Unnamed: 0','id').write.parquet(\"/Volumes/WinDisk/full_open_db.pqt\")\n",
    "# save the table as parquet\n",
    "df_files.withColumnRenamed('Unnamed: 0','id').sample(False, 0.1).write.parquet(\"/Volumes/WinDisk/small_open_db.pqt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "cf_file = glob('/Users/mader/Dropbox/4Quant/Projects/TumorSegmentation/10092825/0013_t1_tse_tra_+c/*.dcm')[0]\n",
    "with open(cf_file,'r') as ifile:\n",
    "    sdata = StringIO(ifile.read())\n",
    "\n",
    "read_file(BytesIO(sdata.readlines()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "sc.binaryFiles('/Users/mader/Dropbox/4Quant/Projects/TumorSegmentation/10092825/0002_t2_blade_tra/*.dcm').mapValues(lambda x: read_file(BytesIO(x))).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pq_context3 = PyqaeContext(sc)\n",
    "dcm_files = pq_context3.readDicomDirectory('/Users/mader/Dropbox/4Quant/Projects/TumorSegmentation/10092825/0002_t2_blade_tra/*.dcm')\n",
    "dcm_files.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_files.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_table = pq_context2.imageTableToDataFrame(dim_files)\n",
    "d_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cpath, c_record in zip(test_table.head()['local_path'],test_table.head().to_records()):\n",
    "    print(cpath,c_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?sc.binaryFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?sc.binaryRecords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "?requests.get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=np.zeros((3,3,3))\n",
    "import urllib\n",
    "urllib.parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import numpy as np\n",
    "try:\n",
    "    import urlparse\n",
    "except: # for python 3\n",
    "    from urllib import parse as urlparse\n",
    "class DRESTAccess(object):\n",
    "    \"\"\"\n",
    "    A distributed access to a REST interface\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_url, fetch_path, def_args, verbose = False):\n",
    "        self.base_url = base_url\n",
    "        self.fetch_path = fetch_path\n",
    "        self.def_args = def_args\n",
    "        self.verbose = verbose\n",
    "        \n",
    "    @staticmethod\n",
    "    def jsonRequest(req_url, args):\n",
    "        response = requests.get(req_url, args)\n",
    "        if response.ok:\n",
    "            return json.loads(response.content)\n",
    "        raise ValueError(\"{} could not be processed correctly\".format(req_url),args)\n",
    "    \n",
    "    @staticmethod\n",
    "    def bufferRequest(req_url, args):\n",
    "        response = requests.get(req_url, args)\n",
    "        if response.ok:\n",
    "            return StringIO(response.content)\n",
    "        raise ValueError(\"{} could not be processed correctly\".format(req_url),args)\n",
    "    \n",
    "    def pull_results(self, **args):\n",
    "        full_url = urlparse.urljoin(self.base_url,self.fetch_path)\n",
    "        new_param = dict(self.def_args + list(args.iteritems()))\n",
    "        print(full_url, new_param)\n",
    "        return DRESTAccess.jsonRequest(full_url, new_param)\n",
    "    \n",
    "    def parallel_pull(self, sc, arg_list, parts = 10):\n",
    "        return sc.parallelize(arg_list).map(lambda x: self.pull_results(**x))\n",
    "    \n",
    "\n",
    "class OpenIDB(DRESTAccess):\n",
    "    def __init__(self, step_count = 50):\n",
    "        self.step_count = step_count\n",
    "        DRESTAccess.__init__(self,\n",
    "                         base_url = \"https://openi.nlm.nih.gov\", \n",
    "                        fetch_path = \"retrieve.php\",\n",
    "                        def_args = [])\n",
    "    \n",
    "    def db_query(self, sc, **args):\n",
    "        base_args = list(args.iteritems())\n",
    "        test_query = self.pull_results(**dict(base_args + [('m',1), ('n',1)]))\n",
    "        m_range = np.arange(1,test_query['total'],self.step_count)\n",
    "        n_range = np.append(m_range[1:],test_query['total'])\n",
    "        qry_rdd = self.parallel_pull(sc, [dict(base_args + [('m',m), ('n',n)]) for m,n in zip(m_range,n_range)])\n",
    "        return qry_rdd.flatMap(lambda x: x['list'])\n",
    "    \n",
    "    @staticmethod\n",
    "    def format_entry(ie):\n",
    "        return {\n",
    "            'uid': ie['uid'],\n",
    "            'major': \";\".join(ie['MeSH']['major']), \n",
    "               'minor': \";\".join(ie['MeSH']['minor']), \n",
    "               'problem': ie['Problems'],\n",
    "              'abstract':ie['abstract'],\n",
    "               'caption':ie['image']['caption'],\n",
    "               'image_id':ie['image']['id'],\n",
    "            'url': ie['imgLarge']\n",
    "              }\n",
    "    \n",
    "    def get_collection(self,sc, coll='cxr', it='xg', lic='byncnd', **args):\n",
    "        \"\"\"\n",
    "        Fetch an entire collection of images as a dataframe\n",
    "        \"\"\"\n",
    "        study_results = self.db_query(sc, **dict(list(args.iteritems())+\n",
    "                                                 [('coll',coll), ('it',it), ('lic', lic)]))\n",
    "        return study_results.map(OpenIDB.format_entry).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "odb = OpenIDB()\n",
    "#odb.pull_results(m=1, n=1, coll='cxr', it='xg', lic='byncnd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = odb.db_query(sc, coll='cxr', it='xg', lic='byncnd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nw_results = odb.get_collection(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nw_results.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nw_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nw_results.registerTempTable(\"LungStudy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext.sql(\"Hey\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
