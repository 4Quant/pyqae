{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducing Deep Learning Pipelines for Apache Spark\n",
    "\n",
    "Deep Learning Pipelines is a new library published by Databricks to provide high-level APIs for scalable deep learning model application and transfer learning via integration of popular deep learning libraries with MLlib Pipelines and Spark SQL. For an overview and the philosophy behind the library, check out the Databricks [blog post](https://databricks.com/blog/2017/06/06/databricks-vision-simplify-large-scale-deep-learning.html). This notebook parallels the [Deep Learning Pipelines README](https://github.com/databricks/spark-deep-learning), detailing usage examples with additional tips for getting started with the library on Databricks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster set-up\n",
    "\n",
    "Deep Learning Pipelines is available as a Spark Package. To use it on your cluster, create a new library with the Source option \"Maven Coordinate\", using \"Search Spark Packages and Maven Central\" to find \"spark-deep-learning\". Then [attach the library to a cluster](https://docs.databricks.com/user-guide/libraries.html). To run this notebook, also create and attach the following libraries: \n",
    "* via PyPI: tensorflow, keras, h5py\n",
    "* via Spark Packages: tensorframes\n",
    "\n",
    "Deep Learning Pipelines is compatible with Spark versions 2.0 or higher and works with any instance type (CPU or GPU)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick User Guide\n",
    "\n",
    "Deep Learning Pipelines provides a suite of tools around working with and processing images using deep learning. The tools can be categorized as\n",
    "* **Working with images** natively in Spark DataFrames\n",
    "* **Transfer learning**, a super quick way to leverage deep learning\n",
    "* **Applying deep learning models at scale**, whether they are your own or known popular models, to image data to make predictions or transform them into features\n",
    "* **Deploying models as SQL functions** to empower everyone by making deep learning available in SQL (coming soon)\n",
    "* **Distributed hyper-parameter tuning** via Spark MLlib Pipelines (coming soon)\n",
    "\n",
    "We'll cover each one with examples below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first get some images to work with in this notebook. We'll use the flowers dataset from the [TensorFlow retraining tutorial](https://www.tensorflow.org/tutorials/image_retraining)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  218M  100  218M    0     0  19.8M      0  0:00:11  0:00:11 --:--:-- 30.7M\n"
     ]
    }
   ],
   "source": [
    "!curl -O http://download.tensorflow.org/example_images/flower_photos.tgz\n",
    "!tar xzf flower_photos.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['flower_photos/roses',\n",
       " 'flower_photos/LICENSE.txt',\n",
       " 'flower_photos/sunflowers',\n",
       " 'flower_photos/daisy',\n",
       " 'flower_photos/tulips',\n",
       " 'flower_photos/dandelion']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glob(os.path.join('flower_photos', '*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['flower_photos/sample/4612075317_91eefff68c_n.jpg',\n",
       " 'flower_photos/sample/9593034725_0062f0d24e_n.jpg',\n",
       " 'flower_photos/sample/517054467_d82d323c33_m.jpg']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's create a small sample set of images for quick demonstrations.\n",
    "img_dir = 'flower_photos'\n",
    "sample_img_dir = \"flower_photos/sample\"\n",
    "os.makedirs(sample_img_dir, exist_ok=True)\n",
    "files = glob(img_dir + \"/tulips/*\")[0:1] + glob(img_dir + \"/daisy/*\")[0:2]\n",
    "for f in files:\n",
    "    shutil.copyfile(f, os.path.join(sample_img_dir, os.path.basename(f)))\n",
    "glob(os.path.join(sample_img_dir, '*'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with images in Spark\n",
    "\n",
    "The first step to applying deep learning on images is the ability to load the images. Deep Learning Pipelines includes utility functions that can load millions of images into a Spark DataFrame and decode them automatically in a distributed fashion, allowing manipulationg at scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages databricks:spark-deep-learning:0.3.0-spark2.2-s_2.11 pyspark-shell'\n",
    "sc = pyspark.SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'PipelinedRDD' object has no attribute 'toDF'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-609df247d52e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msparkdl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mreadImages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mimage_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreadImages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_img_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/spark-a23c7dc6-4fec-4505-9f21-0583a2b92e3f/userFiles-42420d64-9026-401f-ba37-386c874ff689/databricks_spark-deep-learning-0.3.0-spark2.2-s_2.11.jar/sparkdl/image/imageIO.py\u001b[0m in \u001b[0;36mreadImages\u001b[0;34m(imageDirectory, numPartition)\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mimageSchema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m     \"\"\"\n\u001b[0;32m--> 231\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_readImages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimageDirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumPartition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/spark-a23c7dc6-4fec-4505-9f21-0583a2b92e3f/userFiles-42420d64-9026-401f-ba37-386c874ff689/databricks_spark-deep-learning-0.3.0-spark2.2-s_2.11.jar/sparkdl/image/imageIO.py\u001b[0m in \u001b[0;36m_readImages\u001b[0;34m(imageDirectory, numPartition, sc)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_readImages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimageDirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumPartition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[0mdecodeImage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mudf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_decodeImage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimageSchema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m     \u001b[0mimageData\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilesToDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimageDirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumPartitions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnumPartition\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mimageData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"filePath\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecodeImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"fileData\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"image\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/spark-a23c7dc6-4fec-4505-9f21-0583a2b92e3f/userFiles-42420d64-9026-401f-ba37-386c874ff689/databricks_spark-deep-learning-0.3.0-spark2.2-s_2.11.jar/sparkdl/image/imageIO.py\u001b[0m in \u001b[0;36mfilesToDF\u001b[0;34m(sc, path, numPartitions)\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[0mrdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinaryFiles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminPartitions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnumPartitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumPartitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0mrdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'PipelinedRDD' object has no attribute 'toDF'"
     ]
    }
   ],
   "source": [
    "from sparkdl import readImages\n",
    "image_df = readImages(sample_img_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting DataFrame contains a string column named \"filePath\" containing the path to each image file, and a image struct (\"`SpImage`\") column called \"image\" containing the decoded image data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'image_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-f7f189a4624b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'image_df' is not defined"
     ]
    }
   ],
   "source": [
    "display(image_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer learning\n",
    "Deep Learning Pipelines provides utilities to perform transfer learning on images, which is one of the fastest (code and run-time -wise) ways to start using deep learning. Using Deep Learning Pipelines, it can be done in just several lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create training & test DataFrames for transfer learning - this piece of code is longer than transfer learning itself below!\n",
    "from sparkdl import readImages\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "tulips_df = readImages(img_dir + \"/tulips\").withColumn(\"label\", lit(1))\n",
    "daisy_df = readImages(img_dir + \"/daisy\").withColumn(\"label\", lit(0))\n",
    "tulips_train, tulips_test = tulips_df.randomSplit([0.6, 0.4])\n",
    "daisy_train, daisy_test = daisy_df.randomSplit([0.6, 0.4])\n",
    "train_df = tulips_train.unionAll(daisy_train)\n",
    "test_df = tulips_test.unionAll(daisy_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from sparkdl import DeepImageFeaturizer \n",
    "\n",
    "featurizer = DeepImageFeaturizer(inputCol=\"image\", outputCol=\"features\", modelName=\"InceptionV3\")\n",
    "lr = LogisticRegression(maxIter=20, regParam=0.05, elasticNetParam=0.3, labelCol=\"label\")\n",
    "p = Pipeline(stages=[featurizer, lr])\n",
    "\n",
    "p_model = p.fit(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the training step may take a while on Community Edition - try making a smaller training set in that case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how well the model does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "tested_df = p_model.transform(test_df)\n",
    "evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
    "print(\"Test set accuracy = \" + str(evaluator.evaluate(tested_df.select(\"prediction\", \"label\"))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad for a first try with zero tuning! Furthermore, we can look at where we are making mistakes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.functions import expr\n",
    "def _p1(v):\n",
    "  return float(v.array[1])\n",
    "p1 = udf(_p1, DoubleType())\n",
    "\n",
    "df = tested_df.withColumn(\"p_1\", p1(tested_df.probability))\n",
    "wrong_df = df.orderBy(expr(\"abs(p_1 - label)\"), ascending=False)\n",
    "display(wrong_df.select(\"filePath\", \"p_1\", \"label\").limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying deep learning models at scale\n",
    "Spark DataFrames are a natural construct for applying deep learning models to a large-scale dataset. Deep Learning Pipelines provides a set of (Spark MLlib) Transformers for applying TensorFlow Graphs and TensorFlow-backed Keras Models at scale. In addition, popular images models can be applied out of the box, without requiring any TensorFlow or Keras code. The Transformers, backed by the Tensorframes library, efficiently handle the distribution of models and data to Spark workers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying popular image models\n",
    "There are many well-known deep learning models for images. If the task at hand is very similar to what the models provide (e.g. object recognition with ImageNet classes), or for pure exploration, one can use the Transformer `DeepImagePredictor` by simply specifying the model name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sparkdl import readImages, DeepImagePredictor\n",
    "\n",
    "image_df = readImages(sample_img_dir)\n",
    "\n",
    "predictor = DeepImagePredictor(inputCol=\"image\", outputCol=\"predicted_labels\", modelName=\"InceptionV3\", decodePredictions=True, topK=10)\n",
    "predictions_df = predictor.transform(image_df)\n",
    "\n",
    "display(predictions_df.select(\"filePath\", \"predicted_labels\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the `predicted_labels` column shows \"daisy\" as a high probability class for all sample flowers using this base model. However, as can be seen from the differences in the probability values, the neural network has the information to discern the two flower types. Hence our transfer learning example above was able to properly learn the differences between daisies and tulips starting from the base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = p_model.transform(image_df)\n",
    "display(df.select(\"filePath\", (1-p1(df.probability)).alias(\"p_daisy\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For TensorFlow users\n",
    "Deep Learning Pipelines provides a MLlib Transformer that will apply the given TensorFlow Graph to a DataFrame containing a column of images (e.g. loaded using the utilities described in the previous section). Here is a very simple example of how a TensorFlow Graph can be used with the Transformer. In practice, the TensorFlow Graph will likely be restored from files before calling `TFImageTransformer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sparkdl import readImages, TFImageTransformer\n",
    "from sparkdl.transformers import utils\n",
    "import tensorflow as tf\n",
    "\n",
    "image_df = readImages(sample_img_dir)\n",
    "\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    image_arr = utils.imageInputPlaceholder()\n",
    "    resized_images = tf.image.resize_images(image_arr, (299, 299))\n",
    "    # the following step is not necessary for this graph, but can be for graphs with variables, etc\n",
    "    frozen_graph = utils.stripAndFreezeGraph(g.as_graph_def(add_shapes=True), tf.Session(graph=g), [resized_images])\n",
    "      \n",
    "transformer = TFImageTransformer(inputCol=\"image\", outputCol=\"transformed_img\", graph=frozen_graph,\n",
    "                                 inputTensor=image_arr, outputTensor=resized_images,\n",
    "                                 outputMode=\"image\")\n",
    "tf_trans_df = transformer.transform(image_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For Keras users\n",
    "For applying Keras models in a distributed manner using Spark, [`KerasImageFileTransformer`](link_here) works on TensorFlow-backed Keras models. It \n",
    "* Internally creates a DataFrame containing a column of images by applying the user-specified image loading and processing function to the input DataFrame containing a column of image URIs\n",
    "* Loads a Keras model from the given model file path \n",
    "* Applies the model to the image DataFrame\n",
    "\n",
    "The difference in the API from `TFImageTransformer` above stems from the fact that usual Keras workflows have very specific ways to load and resize images that are not part of the TensorFlow Graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the transformer, we first need to have a Keras model stored as a file. For this notebook we'll just save the Keras built-in InceptionV3 model instead of training one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.applications import InceptionV3\n",
    "\n",
    "model = InceptionV3(weights=\"imagenet\")\n",
    "model.save('/tmp/model-full.h5')  # saves to the local filesystem\n",
    "# move to a permanent place for future use\n",
    "dbfs_model_path = 'dbfs:/models/model-full.h5'\n",
    "dbutils.fs.cp('file:/tmp/model-full.h5', dbfs_model_path)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now on the prediction side:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.applications.inception_v3 import preprocess_input\n",
    "from keras.preprocessing.image import img_to_array, load_img\n",
    "import numpy as np\n",
    "from pyspark.sql.types import StringType\n",
    "from sparkdl import KerasImageFileTransformer\n",
    "\n",
    "def loadAndPreprocessKerasInceptionV3(uri):\n",
    "  # this is a typical way to load and prep images in keras\n",
    "  image = img_to_array(load_img(uri, target_size=(299, 299)))  # image dimensions for InceptionV3\n",
    "  image = np.expand_dims(image, axis=0)\n",
    "  return preprocess_input(image)\n",
    "\n",
    "dbutils.fs.cp(dbfs_model_path, 'file:/tmp/model-full-tmp.h5')\n",
    "transformer = KerasImageFileTransformer(inputCol=\"uri\", outputCol=\"predictions\",\n",
    "                                        modelFile='/tmp/model-full-tmp.h5',  # local file path for model\n",
    "                                        imageLoader=loadAndPreprocessKerasInceptionV3,\n",
    "                                        outputMode=\"vector\")\n",
    "\n",
    "files = [\"/dbfs\" + str(f.path)[5:] for f in dbutils.fs.ls(sample_img_dir)]  # make \"local\" file paths for images\n",
    "uri_df = sqlContext.createDataFrame(files, StringType()).toDF(\"uri\")\n",
    "\n",
    "keras_pred_df = transformer.transform(uri_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "display(keras_pred_df.select(\"uri\", \"predictions\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean up data generated for this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources\n",
    "* See the Databricks [blog post](https://databricks.com/blog/2017/06/06/databricks-vision-simplify-large-scale-deep-learning.html) announcing Deep Learning Pipelines for a high-level overview and more in-depth discussion of some of the concepts here.\n",
    "* Check out the [Deep Learning Pipelines github page](https://github.com/databricks/spark-deep-learning).\n",
    "* Learn more about [deep learning on Databricks](https://docs.databricks.com/applications/deep-learning/index.html)."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "name": "Deep Learning Pipelines on Databricks",
  "notebookId": 3954991145914309
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
